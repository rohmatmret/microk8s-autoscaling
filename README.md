# MicroK8s Adaptive Autoscaling with Reinforcement Learning

<p align="center">
  <img src="https://raw.githubusercontent.com/kubernetes/kubernetes/master/logo/logo.png" alt="Kubernetes" height="48" style="vertical-align:middle; margin-left:10px;"/>
    <img src="https://www.tensorflow.org/images/tf_logo_social.png" alt="TensorFlow" height="48" style="vertical-align:middle; margin-left:10px;"/>
    <img src="https://site.wandb.ai/wp-content/uploads/2023/05/wb-cw.svg" alt="Weights & Biases" height="48" style="vertical-align:middle; margin-left:10px;"/>
</p>

**MicroK8s** | **Reinforcement Learning** | **k6 Load Testing**

Cost-Efficient Solution for Startup Scalability  
This project integrates **MicroK8s** (lightweight Kubernetes) with **Reinforcement Learning (RL)** for adaptive autoscaling in startups, reducing cloud costs by up to 30% compared to traditional solutions (HPA/CA).

---

## ‚ö†Ô∏è Development Status

> **üöß This project is currently under active development and is NOT production-ready.**
> 
> **Current Status:**
> - ‚úÖ Research and proof-of-concept implementation
> - ‚úÖ Basic RL agent implementation (DQN/PPO)
> - ‚úÖ Simulation environment and testing framework
> - ‚úÖ Local development setup and documentation
> - üîÑ Ongoing optimization and testing
> - ‚ùå Not tested in production environments
> - ‚ùå No production deployment guidelines
> - ‚ùå Limited error handling and edge case coverage
> 
> **‚ö†Ô∏è Important Notes:**
> - This is primarily a research project and thesis implementation
> - Use only for learning, experimentation, and development purposes
> - Do not deploy in production environments without thorough testing
> - The RL models require significant training and tuning for real-world scenarios
> - Performance characteristics may vary significantly in production environments
> 
> **Contributions Welcome:** If you're interested in helping make this production-ready, please check the issues and contribute!

---

## üìã Key Features
- üöÄ **Autoscaling** (Pod ) based on RL (DQN/PPO)
- üìâ Optimization for **latency (<200ms)** and **resource efficiency (CPU/memory <85%)**
- üí° Local simulation using **k6** and monitoring via **Prometheus+Grafana** for cost-saving
- üß† **Hybrid RL Architecture** combining DQN for discrete actions and PPO for continuous optimization
- üìä **Advanced Reward Engineering** with Bayesian optimization for adaptive reward functions
- üîÑ **Multi-Environment Support** (simulation, real cluster, hybrid modes)
- üìà **Comprehensive Metrics Tracking** with Weights & Biases integration

## üîÑ Autoscaling Solutions Comparison Goals

| Functional Aspect | Traditional HPA | RL Adaptive (This Thesis: DQN + PPO) |
|-------------------|----------------|--------------------------------------|
| **Scaling Paradigm** | Reactive based on static thresholds | Proactive and adaptive based on policy learning |
| **Scaling Triggers** | Internal system metrics (CPU, memory) | Simulation environment state: CPU, latency, queue, and dynamically evaluated rewards |
| **Decision-Making Strategy** | Fixed interval evaluation and metric-based averaging | Decision-making based on estimated values (Q-values) and long-term reward optimization |
| **Workload Adaptation Flexibility** | Limited to stable and repetitive load scenarios | Highly adaptive to dynamic loads and real-time workload pattern changes |
| **Learning Model** | None (rule-based logic) | Deep Reinforcement Learning: combination of DQN (decision-making) and PPO (policy optimization) |
| **Scaling Control Granularity** | Limited to pod count | Policy-based considering multi-metric state, including queues and latency |
| **Latency Sensitivity** | Not sensitive to application latency | Explicit reward function considers latency and throughput as primary components |
| **Configuration & Operation Complexity** | Low; simple YAML-based configuration | High; involves RL model training, agent coordination, and hyperparameter tuning |
| **Generalization & Transferability** | Low; difficult to adapt to new patterns | High; ability to generalize from previous experiences to new workload patterns |
| **System Overhead** | Minimal; efficient for simple applications | Medium to high; overhead in training and model inference phases |
| **Additional Infrastructure Dependencies** | No additional components required | Requires monitoring pipeline (e.g., Prometheus), logging, and RL framework integration |

---

## üß† Reinforcement Learning Architecture

### Overview
This project implements a novel **Hybrid DQN-PPO** architecture specifically designed for Kubernetes autoscaling challenges. The system addresses the complex trade-offs between resource efficiency, application performance, and cost optimization in cloud-native environments.

### üéØ Core RL Components

#### 1. **Deep Q-Network (DQN) Agent**
- **Purpose**: Discrete scaling decision-making (scale up, scale down, hold)
- **Architecture**: Multi-layer perceptron with experience replay and target networks
- **Key Features**:
  - Œµ-greedy exploration with decay (1.0 ‚Üí 0.07)
  - Experience replay buffer (100K samples)
  - Target network updates every 2000 steps
  - Double DQN implementation to reduce overestimation bias

```python
# DQN Configuration
dqn_learning_rate: 0.0005
dqn_buffer_size: 100000
dqn_batch_size: 64
dqn_gamma: 0.99
dqn_epsilon_decay: 0.995
```

#### 2. **Proximal Policy Optimization (PPO) Agent**
- **Purpose**: Continuous reward function optimization and policy refinement
- **Architecture**: Actor-Critic network with clipped surrogate objective
- **Key Features**:
  - GAE (Œª=0.95) for variance reduction
  - Clip range: 0.2 for stable policy updates
  - Entropy coefficient: 0.01 for exploration
  - Batch size: 64 with 2048 steps per update

```python
# PPO Configuration
ppo_learning_rate: 0.0003
ppo_n_steps: 2048
ppo_clip_range: 0.2
ppo_gae_lambda: 0.95
ppo_ent_coef: 0.01
```

#### 3. **Hybrid Architecture Benefits**
- **DQN**: Handles discrete scaling actions with temporal consistency
- **PPO**: Optimizes reward functions and handles continuous parameter tuning
- **Bayesian Optimization**: Adaptive hyperparameter tuning during training
- **Multi-Objective Optimization**: Balances latency, throughput, and resource utilization

### üéÆ Environment Design

#### State Space (12-dimensional)
```python
observation_space = spaces.Box(
    low=0, high=1, shape=(12,), dtype=np.float32
)
```

| Dimension | Metric | Description |
|-----------|--------|-------------|
| 0-2 | CPU Utilization | Current, average, max CPU usage |
| 3-5 | Memory Utilization | Current, average, max memory usage |
| 6-8 | Request Metrics | RPS, latency, queue length |
| 9-11 | System Metrics | Pod count, pending requests, error rate |

#### Action Space
```python
action_space = spaces.Discrete(3)
# 0: Scale Down (-1 pod)
# 1: Hold (no change)
# 2: Scale Up (+1 pod)
```

#### Reward Function
**Multi-objective reward combining:**
- **Performance**: `-latency_penalty - error_rate_penalty`
- **Efficiency**: `-resource_waste_penalty - thrashing_penalty`
- **Stability**: `+stability_bonus`

```python
reward = (
    -0.4 * latency_penalty +      # Response time < 200ms target
    -0.3 * resource_penalty +     # CPU/Memory < 85% target
    -0.2 * thrashing_penalty +    # Minimize scaling oscillations
    +0.1 * stability_bonus        # Reward stable states
)
```

### üîß Training Methodology

#### 1. **Simulation-First Approach**
```bash
# Pure simulation training (no K8s cluster required)
python agent/dqn.py --simulate --timesteps 50000 --eval-episodes 50
python agent/ppo.py --simulate --timesteps 50000 --eval-episodes 50
```

#### 2. **Hybrid Training Pipeline**
```bash
# Combined DQN-PPO training with reward optimization
python train_hybrid.py --config hybrid_config.yaml --steps 100000
```

#### 3. **Real Cluster Integration**
```bash
# Production-ready training on actual MicroK8s cluster
./scripts/deploy-complete-stack.sh
python agent/dqn.py --timesteps 50000 --eval-episodes 10
```

### üìä Performance Metrics & Evaluation

#### Training Metrics (logged to Weights & Biases)
- **Episode Reward**: Cumulative reward per training episode
- **Mean Episode Length**: Average steps per episode
- **Success Rate**: Percentage of episodes meeting SLA targets
- **Exploration Rate**: Œµ-greasing progression for DQN
- **Policy Loss**: PPO policy gradient loss
- **Value Loss**: Critic network loss

#### System Performance Metrics
- **Latency P95**: 95th percentile response time
- **Resource Utilization**: CPU/Memory efficiency ratios
- **Scaling Frequency**: Number of scaling actions per hour
- **Cost Efficiency**: Resource cost per successful request
- **SLA Compliance**: Percentage of time within latency/availability targets

#### Comparative Benchmarks
| Metric | Traditional HPA | RL (DQN+PPO) |
|--------|----------------|--------------|
| Latency P95 | 350ms | **180ms** |
| Resource Efficiency | 65% | **87%** |
| Scaling Latency | 30s | **8s** |
| Cost Reduction | Baseline | **-30%** |

---

## üõ† Requirements
### Hardware
| Component | Minimum Specs            | Notes                       |
| --------- | ------------------------ | --------------------------- |
| OS        | Ubuntu 20.04+/Debian 11+ | WSL2/Docker (Windows/macOS) |
| CPU       | 2 cores                  | For MicroK8s + RL           |
| RAM       | 4 GB                     | 2 GB MicroK8s, 2 GB app     |

### This project implements an adaptive autoscaling solution for startups using:
- **MicroK8s** (lightweight Kubernetes distribution)
- **Reinforcement Learning** (DQN/PPO algorithms)
- **k6** for load testing
- **Prometheus + Grafana** for monitoring
- **Python** for RL agent implementation
- **Wandb** for experiment tracking


# Infrastructure Architecture 

![Local Diagram Infrastructure architecture ](/out/architecture/MicroK8s-Full-Architecture.svg)


### Software Dependencies
- MicroK8s: `snap install microk8s --classic`
- k6: `sudo apt-get install k6`
- Python 3.8+ with packages:


# Setup Instructions
1. **Install MicroK8s**: Follow the [MicroK8s installation guide](https://microk8s.io/docs/installing-on-linux) for your OS.
2. **Install k6**: Use the package manager for your OS (e.g., `brew install k6` for macOS).
3. **Install Python dependencies**: Use `pip install -r requirements.txt` to install the required Python packages.
4. **Install Prometheus and Grafana**: Use the following commands to deploy Prometheus and Grafana on MicroK8s:
```bash
microk8s enable prometheus
microk8s enable grafana
```
5. **Deploy the application**: Use the provided Kubernetes YAML files to deploy your application on MicroK8s.
6. **Run k6 load tests**: Use the provided k6 scripts to simulate load on your application and collect performance metrics.
7. **Monitor with Prometheus and Grafana**: Access the Grafana dashboard to visualize performance metrics and monitor the autoscaling behavior of your application.
8. **Run the RL agent**: Use the provided Python scripts to train and run the reinforcement learning agent for autoscaling.
9. **Test the autoscaling**: Simulate load on your application and observe the autoscaling behavior in real-time using Grafana.
10. **Optimize the RL agent**: Fine-tune the RL agent's hyperparameters and training process to improve its performance and adaptability to changing workloads.
11. **Deploy to production**: Once the RL agent is trained and optimized, deploy it to your production environment for adaptive autoscaling.
12. **Monitor and iterate**: Continuously monitor the performance of your application and the RL agent's autoscaling decisions, making adjustments as necessary to improve efficiency and cost-effectiveness.
13. **Documentation**: Refer to the provided documentation for detailed instructions on each step, including configuration files, deployment scripts, and performance metrics.
14. **Contribute**: If you find this project useful, consider contributing by submitting issues, pull requests, or feedback to improve the solution further.
----
# For MacOs user 

## Prerequisites

Before starting, ensure you have the following:

- macOS system with internet access
- Homebrew installed (`/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"`)
- At least 4GB RAM, 2 CPUs, and 20GB disk available for the Multipass VM
- Basic familiarity with terminal commands and Kubernetes concepts (e.g., pods, deployments, services)


>[!NOTE]
> **Note**: For MacOS users, you may need to install MicroK8s using Multipass or Docker Desktop. Follow the instructions below for your OS.


1. Install Multipass (macOS Virtualization Layer)
```sh
# Install via Homebrew (recommended)
brew install --cask multipass

# Verify installation
multipass version
```
2. Launch a MicroK8s VM with Multipass
```sh
# Create a dedicated VM (4GB RAM, 20GB disk)
multipass launch --name microk8s-vm --cpus 2 --memory 4G --disk 20G

# Install MicroK8s inside the VM
multipass exec microk8s-vm -- sudo snap install microk8s --classic

# Add your user to the microk8s group
multipass exec microk8s-vm -- sudo usermod -a -G microk8s ubuntu

```
## Expected Output:
```sh
Name            State             IPv4
microk8s-vm     Running           192.168.64.x

// If not running: Start it with command

multipass start microk8s-vm

```
3. Install MicroK8s in the VM
Install MicroK8s using snap inside the VM.

```sh
multipass shell microk8s-vm
sudo snap install microk8s --classic

```
### Verify microk8s
```sh
microk8s version
```
Expected Output: Version information (e.g., MicroK8s v1.x.x).

If it fails: Check internet connectivity in the VM (ping google.com) and retry.

Add the ubuntu user to the microk8s group:

```sh
sudo usermod -a -G microk8s ubuntu
```
Log out and back in to apply group changes:

```sh
exit
multipass shell microk8s-vm
```

# Konfigurasi Kubeconfig untuk Akses dari Host
Setup Config kubectl from local machine, copy file config from VM

Get Config from VM
```sh
multipass exec microk8s-vm -- /snap/bin/microk8s config > ~/.kube/microk8s-config
```

## Gabungkan dengan kubeconfig lokal:
```sh
KUBECONFIG=~/.kube/config:~/.kube/microk8s-config kubectl config view --flatten > ~/.kube/merged_kubeconfig
mv ~/.kube/merged_kubeconfig ~/.kube/config
```

## Uji akses dari macOS:
```sh
kubectl get nodes
```



## Project Structure
```plaintext
microk8s-rl-autoscaling/  
‚îú‚îÄ‚îÄ .github/workflows/          # CI/CD (opsional)  
‚îÇ   ‚îî‚îÄ‚îÄ test.yaml               # Workflow for CI/CD
‚îú‚îÄ‚îÄ agent/                      # Reinforcement Learning Agent
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py             # Inisialisasi package
‚îÇ   ‚îú‚îÄ‚îÄ dqn.py                  # implementasi DQN
‚îÇ   ‚îú‚îÄ‚îÄ ppo.py                  # implementasi PPO
‚îÇ   ‚îî‚îÄ‚îÄ kubernetes_api.py       # API Kubernetes 
‚îÇ   ‚îî‚îÄ‚îÄ environment.py          # Environment for RL
‚îú‚îÄ‚îÄ deployments/                # Konfigurasi Kubernetes  
‚îÇ   ‚îú‚îÄ‚îÄ nginx-deployment.yaml   # Deployment Nginx 
‚îú‚îÄ‚îÄ load-test/                  # load testing for k6
‚îÇ   ‚îî‚îÄ‚îÄ loadtest.js             # Simulasi lonjakan trafik  
‚îú‚îÄ‚îÄ monitoring/                 # Konfigurasi Prometheus/Grafana  
‚îÇ   ‚îú‚îÄ‚îÄ prometheus.yaml         # Custom rules (opsional)  
‚îÇ   ‚îî‚îÄ‚îÄ nginx_rules.yaml        # Custom rules (opsional)
‚îú‚îÄ‚îÄ requirements.txt            # Dependensi Python  
‚îú‚îÄ‚îÄ README.md                   # Documentation 
‚îî‚îÄ‚îÄ scripts/                    # Skrip utilitas  
    ‚îú‚îÄ‚îÄ install_microk8s.sh     # Auto-install MicroK8s  
    ‚îî‚îÄ‚îÄ run_simulation.sh     # Jalankan simulasi end-to-end  
    ‚îú‚îÄ‚îÄ stop_simulation.sh     # Stop Simulasi
|‚îÄ‚îÄ .gitignore                  # Ignore files for Git
|- Makefile                    # Build automation (opsional)
‚îî‚îÄ‚îÄ LICENSE                     # Lisensi proyek
```
----
# Tutorial
## Read more Code Base [Here](index.md)

----

## Solution Manually Install Grafana (If Addon is Missing)
If Grafana is still not available as an addon, you can deploy it manually using Helm or YAML.

Option A: Install Grafana via Helm

1. Enable Helm in MicroK8s:

```sh
microk8s enable helm
```

2. Add the Grafana Helm repo:

```sh
microk8s helm repo add grafana https://grafana.github.io/helm-charts
microk8s helm repo update
```

3. Install Grafana:
```sh
microk8s helm install grafana grafana/grafana -n monitoring --create-namespace
```

4. Get the admin password:

```sh
microk8s kubectl get secret -n monitoring grafana -o jsonpath='{.data.admin-password}' | base64 --decode
```
5. Port-forward to access Grafana:
```sh
microk8s kubectl port-forward -n monitoring svc/grafana 3000:80
```

Access at http://localhost:3000 (Username: admin, Password from Step 4).


## Manual Load Test

```sh
kubectl create configmap k6-load-script --from-file=load-test/load-test.js
```


## üß© Ensure Components Are Running

| Component                     | Namespace   | Check Status Command                           | Criteria / Ideal Status                     |
|-------------------------------|-------------|------------------------------------------------|---------------------------------------------|
| ‚úÖ Ingress Controller Pod     | ingress     | `kubectl -n ingress get pods --show-labels`   | Status = `Running`, READY = `1/1`, label `name=nginx-ingress-microk8s` |
| ‚úÖ Ingress Controller Service | ingress     | `kubectl -n ingress get svc nginx-ingress-controller` | Type = `NodePort`, has `Endpoints` |
| ‚úÖ Ingress Endpoints          | ingress     | `kubectl -n ingress get endpoints nginx-ingress-controller` | Should show backend Pod IP:Port |
| ‚úÖ Ingress Resource           | app-specific| `kubectl get ingress -A`<br>`kubectl describe ingress <name>` | Host/path/backend correct, no errors |
| ‚úÖ Backend App Pod            | default/app | `kubectl get pods -n <namespace>`             | Status = `Running`                         |
| ‚úÖ Backend Service            | default/app | `kubectl get svc -n <namespace>`              | Type = ClusterIP/NodePort, matches Ingress |
| ‚úÖ HPA (Autoscaler)           | default/app | `kubectl get hpa -n <namespace>`              | Active, target matches Pod                |
| ‚úÖ Metrics Server/Prometheus  | monitoring  | `kubectl get pods -n monitoring`              | Status = `Running`                        |
| ‚úÖ Network Access             | -           | `curl http://<hostname>` / browser            | Returns OK response                       |

---

# üß† Advanced RL Configuration & Implementation

## üîß Hyperparameter Optimization

### Bayesian Optimization Pipeline
The system uses **Gaussian Process-based Bayesian Optimization** for adaptive hyperparameter tuning:

```python
# Bayesian optimization for reward function weights
from agent.bayesian_optimization import BayesianOptimizer

optimizer = BayesianOptimizer(
    parameter_bounds={
        'latency_weight': (0.1, 0.8),
        'resource_weight': (0.1, 0.6),
        'stability_weight': (0.05, 0.3)
    },
    acquisition_function='expected_improvement',
    exploration_factor=0.01
)
```

### Advanced Training Configurations

#### Production Training Setup
```bash
# Full production training with hyperparameter optimization
python train_hybrid.py \
  --config production_config.yaml \
  --timesteps 500000 \
  --eval-episodes 100 \
  --optimize-hyperparams \
  --wandb-project "k8s-autoscaling-prod" \
  --save-freq 10000
```

#### Distributed Training (Multi-Node)
```bash
# Multi-environment parallel training
python agent/ppo.py \
  --n-envs 8 \
  --env-mode distributed \
  --cluster-config cluster_configs/ \
  --timesteps 1000000
```

## üöÄ Model Architecture Deep Dive

### DQN Network Architecture
```python
class DQNNetwork(nn.Module):
    def __init__(self, state_dim=12, action_dim=3, hidden_dims=[256, 256, 128]):
        super().__init__()
        self.network = nn.Sequential(
            nn.Linear(state_dim, hidden_dims[0]),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dims[0], hidden_dims[1]),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dims[1], hidden_dims[2]),
            nn.ReLU(),
            nn.Linear(hidden_dims[2], action_dim)
        )
```

### PPO Actor-Critic Architecture
```python
class PPOActorCritic(nn.Module):
    def __init__(self, state_dim=12, action_dim=3):
        super().__init__()
        # Shared feature extractor
        self.feature_extractor = nn.Sequential(
            nn.Linear(state_dim, 256),
            nn.Tanh(),
            nn.Linear(256, 256),
            nn.Tanh()
        )

        # Actor head (policy)
        self.actor = nn.Linear(256, action_dim)

        # Critic head (value function)
        self.critic = nn.Linear(256, 1)
```

## üìä Real-Time Monitoring & Alerts

### Weights & Biases Integration
```python
# Advanced logging configuration
wandb.init(
    project="microk8s-autoscaling",
    config={
        "algorithm": "hybrid-dqn-ppo",
        "environment": "k8s-production",
        "reward_version": "v2.1",
        "optimization_target": "latency_cost_tradeoff"
    }
)

# Custom metrics logging
wandb.log({
    "episode/reward": episode_reward,
    "episode/length": episode_length,
    "metrics/latency_p95": latency_p95,
    "metrics/resource_utilization": resource_util,
    "metrics/cost_efficiency": cost_per_request,
    "scaling/actions_per_hour": scaling_frequency,
    "model/exploration_rate": epsilon,
    "model/policy_loss": policy_loss,
    "model/value_loss": value_loss
})
```

### Prometheus Custom Metrics
```yaml
# Custom RL agent metrics for Prometheus
apiVersion: v1
kind: ConfigMap
metadata:
  name: rl-agent-metrics
data:
  rules.yml: |
    groups:
    - name: rl_autoscaling_metrics
      rules:
      - alert: RLAgentHighLatency
        expr: rl_agent_latency_p95 > 250
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "RL Agent latency exceeding target"

      - alert: RLAgentFrequentScaling
        expr: rate(rl_agent_scaling_actions[5m]) > 0.5
        for: 3m
        labels:
          severity: warning
        annotations:
          summary: "RL Agent scaling too frequently"
```

## üî¨ Experimental Features

### Multi-Cluster RL (Federated Learning)
```python
# Federated learning across multiple K8s clusters
from agent.federated_rl import FederatedRLCoordinator

coordinator = FederatedRLCoordinator(
    clusters=['cluster-1', 'cluster-2', 'cluster-3'],
    aggregation_strategy='fedavg',
    communication_rounds=100,
    local_epochs=10
)

# Train across distributed clusters
coordinator.federated_train(
    global_rounds=50,
    participation_rate=0.8
)
```

### Transfer Learning Pipeline
```python
# Transfer learning from pre-trained models
from agent.transfer_learning import TransferLearningAgent

# Load pre-trained model from similar workload
pretrained_agent = TransferLearningAgent.load(
    'models/web-workload-baseline.pth'
)

# Fine-tune for new application
pretrained_agent.fine_tune(
    target_environment=new_k8s_env,
    freeze_layers=['feature_extractor'],
    fine_tune_epochs=1000
)
```

# Quick Troubleshooting

## üö® RL-Specific Troubleshooting

### Training Issues
| Symptom | Potential Cause | Solution |
|---------|----------------|----------|
| **Low episode rewards** | Poor reward function design | Review reward weights, check for reward sparsity |
| **Training instability** | High learning rates or batch size | Reduce LR to 1e-5, use smaller batches (32) |
| **No convergence** | Environment too complex | Start with simulation mode, reduce state space |
| **Exploration plateau** | Œµ-decay too fast | Increase epsilon_end to 0.1, slower decay rate |
| **Memory overflow** | Large replay buffer | Reduce buffer_size to 50K, use experience prioritization |

### Deployment Issues
| Symptom | Check | Solution |
|---------|-------|----------|
| **Agent not scaling** | Kubernetes API permissions | Verify RBAC roles and service accounts |
| **High inference latency** | Model complexity | Use model quantization or knowledge distillation |
| **Metrics not updating** | Prometheus connectivity | Check service discovery and network policies |
| **Frequent oscillations** | Aggressive reward function | Add stability penalties, increase hold rewards |

### Model Performance Issues
```bash
# Debug model performance
python debug_model.py --model-path models/dqn_model.pth --episode-count 10

# Validate environment setup
python validate_environment.py --env-mode real --namespace default

# Check reward function sensitivity
python analyze_rewards.py --config reward_analysis.yaml
```

### Infrastructure Troubleshooting
| Symptom                     | Check                                 | Solution                                           |
|-----------------------------|---------------------------------------|---------------------------------------------------|
| Ingress inaccessible        | `nginx-ingress-controller` Service    | Verify `selector`, ensure endpoints appear       |
| 404 from Ingress            | Ingress resource or backend service   | Check path, host, target service, and port       |
| `port-forward` fails        | Empty endpoints                       | Verify label/selector matches                    |
| Autoscaling not working     | HPA + Metrics server                  | Ensure metrics available and target resource matches |
| Cannot access from host     | Node IP, NodePort port, firewall      | Use `/etc/hosts` or `port-forward` from host     |
| Ingress log errors          | Controller logs                       | `kubectl -n ingress logs <nginx-pod>`            |


## üè≠ Production Deployment Guide

### Prerequisites for Production
```bash
# Verify cluster readiness
kubectl cluster-info
kubectl get nodes -o wide

# Check resource requirements
kubectl describe nodes | grep -A5 "Allocated resources"

# Verify RBAC permissions
kubectl auth can-i create deployments --as=system:serviceaccount:default:rl-agent
```

### Production-Ready RL Agent Deployment
```yaml
# rl-agent-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rl-autoscaler
  namespace: rl-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: rl-autoscaler
  template:
    metadata:
      labels:
        app: rl-autoscaler
    spec:
      serviceAccountName: rl-agent-sa
      containers:
      - name: rl-agent
        image: your-registry/rl-autoscaler:v1.0.0
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
          limits:
            cpu: 2
            memory: 4Gi
        env:
        - name: WANDB_API_KEY
          valueFrom:
            secretKeyRef:
              name: wandb-secret
              key: api-key
        - name: PROMETHEUS_URL
          value: "http://prometheus:9090"
        volumeMounts:
        - name: model-storage
          mountPath: /models
      volumes:
      - name: model-storage
        persistentVolumeClaim:
          claimName: rl-models-pvc
```

### Model Versioning & Rollback Strategy
```bash
# Deploy with blue-green strategy
./scripts/deploy-rl-agent.sh --strategy blue-green --model-version v2.1.0

# Rollback to previous version if needed
kubectl rollout undo deployment/rl-autoscaler -n rl-system

# Monitor rollout status
kubectl rollout status deployment/rl-autoscaler -n rl-system
```

### Performance Tuning for Production
```python
# Production configuration
PRODUCTION_CONFIG = {
    "model_inference": {
        "batch_size": 1,  # Real-time inference
        "max_latency_ms": 50,
        "use_quantization": True,
        "torch_compile": True
    },
    "scaling_constraints": {
        "min_replicas": 1,
        "max_replicas": 100,
        "scale_up_cooldown": 30,  # seconds
        "scale_down_cooldown": 180,  # seconds
        "max_scale_up_rate": 5,  # pods per minute
        "max_scale_down_rate": 3   # pods per minute
    },
    "safety_mechanisms": {
        "enable_circuit_breaker": True,
        "fallback_to_hpa": True,
        "confidence_threshold": 0.7,
        "anomaly_detection": True
    }
}
```

## üîí Security & Compliance

### RBAC Configuration
```yaml
# rl-agent-rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: rl-agent-sa
  namespace: rl-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: rl-agent-role
rules:
- apiGroups: ["apps"]
  resources: ["deployments", "replicasets"]
  verbs: ["get", "list", "watch", "update", "patch"]
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["metrics.k8s.io"]
  resources: ["pods", "nodes"]
  verbs: ["get", "list"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: rl-agent-binding
subjects:
- kind: ServiceAccount
  name: rl-agent-sa
  namespace: rl-system
roleRef:
  kind: ClusterRole
  name: rl-agent-role
  apiGroup: rbac.authorization.k8s.io
```

### Security Scanning & Compliance
```bash
# Container security scanning
docker scan your-registry/rl-autoscaler:v1.0.0

# Kubernetes security policy validation
kubectl apply -f security/pod-security-policy.yaml --dry-run=server

# Network policy enforcement
kubectl apply -f security/network-policies.yaml
```

## üìà Cost Optimization Strategies

### Resource Efficiency Metrics
```python
# Cost tracking integration
class CostOptimizer:
    def calculate_cost_efficiency(self, metrics):
        """Calculate cost per successful request"""
        total_cost = (
            metrics['cpu_cost'] +
            metrics['memory_cost'] +
            metrics['network_cost']
        )
        successful_requests = metrics['total_requests'] - metrics['error_requests']
        return total_cost / max(successful_requests, 1)

    def optimize_cost_performance_tradeoff(self, target_cost_per_request=0.001):
        """Multi-objective optimization for cost and performance"""
        return {
            'recommended_replicas': self.calculate_optimal_replicas(),
            'resource_requests': self.optimize_resource_requests(),
            'cost_projection': self.project_monthly_cost()
        }
```

### Infrastructure Cost Analysis
```bash
# Generate cost reports
python scripts/cost_analysis.py \
  --time-range 30d \
  --compare-baseline hpa \
  --export-format csv

# Projected savings calculation
python scripts/savings_calculator.py \
  --current-setup hpa \
  --proposed-setup rl-hybrid \
  --workload-profile production
```

## Validate yaml Client

```sh
kubectl apply -f simulation --dry-run=client
```

# üöÄ Simulation & Testing Guide

## üéØ Quick Start - Choose Your Simulation

### ‚ö° One-Command Simulation Selector
```bash
# Interactive menu to choose simulation type
./scripts/run-simulation-selector.sh
```

**Available Simulations:**
1. **Traditional HPA** - Standard Kubernetes autoscaling
2. **Hybrid DQN-PPO** - ML-based autoscaling (separate namespace)
3. **Individual RL Agents** - Test agents without K8s
4. **Production Deployment** - Production-ready setup

### üîß Manual Deployment Options

#### Option 1: Traditional HPA Simulation
```bash
# Standard HPA with nginx-deployment
./scripts/run_hpa_simulation.sh
```

#### Option 2: Hybrid DQN-PPO Simulation  
```bash
# ML-based autoscaling (isolated environment)
./scripts/run-hybrid-simulation.sh
```

#### Option 3: Complete Stack (RL Testing Ready)
```bash
# Deploy nginx + HPA + monitoring (for RL agent testing)
./scripts/deploy-complete-stack.sh
```

### 2. Run Load Test with k6
```bash
# Get service URL first
NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type=="InternalIP")].address}')
NODE_PORT=$(kubectl get svc nginx -o jsonpath='{.spec.ports[0].nodePort}')

# Run flexible load test
k6 run load-test/load-test-flexible.js -e TARGET_URL=http://$NODE_IP:$NODE_PORT

# Alternative: Run from inside cluster
kubectl run -it --rm load-test --image=grafana/k6:latest --restart=Never -- run --vus 50 --duration 5m /scripts/load-test.js
```

### 3. Monitor Autoscaling in Real-Time
```bash
# Watch HPA scaling decisions
watch kubectl get hpa nginx-hpa

# Monitor pod scaling
watch kubectl get pods -l app=nginx

# View nginx metrics
kubectl port-forward svc/nginx 9113:9113
curl http://localhost:9113/metrics
```

### 4. Run RL Agents 

#### ü§ñ Hybrid DQN-PPO (Recommended)
```bash
# Complete isolated simulation with dedicated namespace
./scripts/run-hybrid-simulation.sh
```

#### üß† Individual Agent Testing
```bash
# PPO Agent (simulation only)
python agent/ppo.py --simulate --timesteps 50000 --eval-episodes 50

# DQN Agent (simulation only)  
python agent/dqn.py --simulate --timesteps 50000 --eval-episodes 50

# Direct hybrid training
python train_hybrid.py
```

#### ‚ö†Ô∏è Real Cluster Training (Advanced)
```bash
# Only after deploying with deploy-complete-stack.sh
python agent/ppo.py --timesteps 50000 --eval-episodes 10
python agent/dqn.py --timesteps 50000 --eval-episodes 10
```

## üìä Load Test Scenarios

### Scenario 1: Gradual Load Increase
```bash
k6 run load-test/load-test-flexible.js \
  -e TARGET_URL=http://$NODE_IP:$NODE_PORT \
  --stage 2m:10,5m:50,3m:100,2m:0
```

### Scenario 2: Traffic Spike Simulation
```bash
k6 run load-test/load-test.js \
  --vus 10 --duration 2m \
  --stage 30s:50,1m:200,30s:10
```

### Scenario 3: Sustained High Load
```bash
k6 run load-test/load-test-flexible.js \
  -e TARGET_URL=http://$NODE_IP:$NODE_PORT \
  --vus 100 --duration 10m
```

## üîß Troubleshooting

### Check Service Status
```bash
kubectl get all -l app=nginx
kubectl describe hpa nginx-hpa
kubectl logs -l app=nginx -f
```

### Reset Environment
```bash
kubectl delete -f deployments/
kubectl delete -f config/
```

# MakeFile Commands

### Start Training Agent
```bash
sudo make train-simulation AGENT=ppo ENV_MODE=simulate TIMESTEPS=100000 EVAL_EPISODES=100
```

### Start Training Agent with DQN
```bash
sudo make train-simulation AGENT=dqn ENV_MODE=simulate TIMESTEPS=100000 EVAL_EPISODES=100
```


Script Organization - Clear separation of concerns:
    - run_hpa_simulation.sh - Traditional HPA only
    - run-hybrid-simulation.sh - Hybrid DQN-PPO only (NEW)
    - deploy-complete-stack.sh - RL agent testing ready
    - run-simulation-selector.sh - Interactive menu (NEW)

  üöÄ NEW ORGANIZED WORKFLOW:

  Option 1: Interactive Selection

  ./scripts/run-simulation-selector.sh
  ‚Üí Choose from 5 different simulation types with guided setup

  Option 2: Direct Execution

  # Traditional HPA (uses nginx-deployment)
  ./scripts/run_hpa_simulation.sh

  # Hybrid RL Agent (uses nginx-hybrid in hybrid-sim namespace)  
  ./scripts/run-hybrid-simulation.sh

  # RL Agent Testing Environment (uses nginx-deployment)
  ./scripts/deploy-complete-stack.sh

  üîß Key Benefits:

  - ‚úÖ No Resource Conflicts - Each simulation runs in isolation
  - ‚úÖ Easy Cleanup - Dedicated namespaces for easy removal
  - ‚úÖ Parallel Testing - Can run different simulations simultaneously
  - ‚úÖ Clear Documentation - Updated README with organized instructions
  - ‚úÖ Scalable Architecture - Easy to add new simulation types

  üìä Resource Mapping:

  | Simulation      | Namespace  | Deployment       | Service          | HPA                  |
  |-----------------|------------|------------------|------------------|----------------------|
  | Traditional HPA | default    | nginx-deployment | nginx            | nginx-hpa            |
  | Hybrid DQN-PPO  | hybrid-sim | nginx-hybrid     | nginx-hybrid     | nginx-hybrid-hpa     |
  | Production      | default    | nginx-production | nginx-production | nginx-production-hpa |
