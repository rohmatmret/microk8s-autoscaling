% JTIIK Paper - Practical Implementation Focus (Indonesian/English)
% Jurnal Teknologi Informasi dan Ilmu Komputer (Universitas Brawijaya)
% SINTA 2

\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[english,bahasa]{babel}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{cite}
\usepackage{url}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{float}
\usepackage[margin=2.5cm]{geometry}
\usepackage{listings}
\usepackage{xcolor}

% Code listing style
\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    backgroundcolor=\color{gray!10}
}

% Title
\title{\textbf{Implementasi Autoscaling Proaktif pada Kubernetes Menggunakan Deep Reinforcement Learning untuk Optimasi Biaya dan Kualitas Layanan}}

% Authors
\author{
Rohmat\textsuperscript{1,*}, Mauridhi Hery Purnomo\textsuperscript{2}, Feby Artwodini Muqtadiroh\textsuperscript{3}\\
\textsuperscript{1}Program Studi Magister Inovasi Sistem dan Teknologi, Sekolah Interdisiplin Manajemen dan Teknologi, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia\\
\textsuperscript{2}Departemen Teknik Elektro, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia\\
\textsuperscript{3}Departemen Sistem Informasi, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia\\
\textsuperscript{*}Email korespondensi: rohmat771@gmail.com
}

\date{}

\begin{document}

\maketitle

\begin{abstract}
Autoscaling adalah mekanisme penting dalam sistem kontainerisasi seperti Kubernetes untuk menjaga kualitas layanan sambil mengoptimalkan biaya operasional. Pendekatan tradisional seperti Horizontal Pod Autoscaler (HPA) menggunakan mekanisme reaktif berbasis ambang batas yang seringkali gagal mengantisipasi perubahan beban kerja dinamis, mengakibatkan pelanggaran Service Level Agreement (SLA) atau pemborosan sumber daya. Penelitian ini mengimplementasikan pendekatan proaktif berbasis deep reinforcement learning yang mengombinasikan Deep Q-Network (DQN) untuk keputusan scaling diskrit dan Proximal Policy Optimization (PPO) untuk optimasi reward berkelanjutan. Sistem hybrid ini menggunakan optimasi multi-objektif yang mempertimbangkan utilisasi CPU, waktu respons, dan efisiensi biaya secara simultan. Evaluasi komprehensif dilakukan pada lingkungan simulasi Kubernetes dengan 5 skenario trafik beragam (beban stabil, peningkatan bertahap, lonjakan tiba-tiba, pola harian, dan periode idle) dalam total 2.202 langkah simulasi. Menggunakan target CPU 70\% yang sama dengan HPA (untuk perbandingan yang adil), agen hybrid mencapai pengurangan 7,0\% dalam pelanggaran SLA (1.067.836 vs 1.148.450), peningkatan 2,4\% dalam waktu respons rata-rata (122ms vs 125ms), dan penurunan biaya operasional 4,5\% (\$2.985.612 vs \$3.126.114). Hasil penelitian memvalidasi bahwa reinforcement learning dapat secara efektif menyeimbangkan performa dan efisiensi melalui pengambilan keputusan proaktif berbasis pembelajaran pola. Implementasi ini memberikan kontribusi praktis berupa arsitektur hybrid RL yang siap diterapkan pada lingkungan produksi Kubernetes dengan panduan konfigurasi lengkap dan analisis trade-off biaya-performa untuk startup dan perusahaan menengah.

\textbf{Kata Kunci:} reinforcement learning, deep Q-network, proximal policy optimization, Kubernetes autoscaling, optimasi biaya, Service Level Agreement
\end{abstract}

\section{Pendahuluan}

Aplikasi cloud-native yang di-deploy melalui platform orkestrasi kontainer seperti Kubernetes harus menyesuaikan sumber daya komputasi secara dinamis untuk menjaga jaminan performa sambil meminimalkan biaya operasional \cite{kubernetes2023hpa}. Tantangan manajemen sumber daya ini menjadi semakin kritis dalam kondisi beban kerja yang bervariasi, di mana mekanisme autoscaling reaktif tradisional seringkali menunjukkan perilaku suboptimal.

\subsection{Latar Belakang}

Kubernetes Horizontal Pod Autoscaler (HPA) merupakan standar industri untuk autoscaling kontainer, menggunakan kebijakan berbasis ambang batas yang memicu aksi scaling ketika metrik melebihi batas yang telah ditentukan (misalnya, utilisasi CPU $>$ 70\%) \cite{kubernetes2023hpa}. Namun, pendekatan reaktif ini memiliki beberapa keterbatasan:

\begin{enumerate}
    \item \textbf{Respon tertunda}: Waktu respons lambat terhadap lonjakan trafik mendadak karena interval pengumpulan metrik
    \item \textbf{Ketidakmampuan prediksi}: Tidak dapat mengantisipasi pola beban kerja
    \item \textbf{Perilaku osilasi}: Fluktuasi scaling di dekat batas ambang
    \item \textbf{Optimasi tunggal}: Mengabaikan trade-off antara performa, biaya, dan kepatuhan SLA
\end{enumerate}

Untuk startup dan perusahaan menengah dengan anggaran terbatas, keterbatasan ini mengakibatkan:
\begin{itemize}
    \item \textbf{Over-provisioning}: Pemborosan hingga 30-40\% sumber daya untuk mengantisipasi spike
    \item \textbf{Under-provisioning}: Pelanggaran SLA yang berakibat hilangnya revenue dan kepercayaan pelanggan
    \item \textbf{Biaya operasional tinggi}: Alokasi sumber daya tidak efisien sepanjang waktu
\end{itemize}

\subsection{Motivasi Penelitian}

Kemajuan terkini dalam deep reinforcement learning (RL) menawarkan solusi prometis untuk strategi autoscaling adaptif yang mempelajari kebijakan scaling langsung dari feedback sistem \cite{mao2016resource, xu2021adaptive, islam2021performance}. Beberapa survei literatur telah mendokumentasikan efektivitas RL dalam autoscaling cloud \cite{zhang2021deep, gari2020reinforcement}, menunjukkan peningkatan signifikan dibanding pendekatan tradisional. Namun, sebagian besar pendekatan berbasis RL yang ada memiliki keterbatasan:

\begin{enumerate}
    \item Optimasi objektif tunggal (hanya efisiensi CPU)
    \item Kurangnya kesadaran SLA dalam fungsi reward
    \item Evaluasi terbatas yang tidak merepresentasikan beban kerja produksi
    \item Tidak ada perbandingan adil dengan metode tradisional
\end{enumerate}

\subsection{Kontribusi Penelitian}

Penelitian ini mengimplementasikan dan mengevaluasi pendekatan \textbf{hybrid DQN-PPO} yang dirancang khusus untuk autoscaling Kubernetes dengan kesadaran SLA. Kontribusi utama meliputi:

\begin{enumerate}
    \item \textbf{Implementasi Arsitektur Hybrid}: Sistem dual-agent yang mengombinasikan DQN untuk aksi scaling diskrit dan PPO untuk optimasi reward adaptif dengan Bayesian hyperparameter tuning.

    \item \textbf{Framework Multi-Objektif}: Fungsi reward komprehensif yang menyeimbangkan efisiensi CPU (target 70\%), optimasi waktu respons (SLA $<$ 200ms), dan minimalisasi biaya operasional.

    \item \textbf{Panduan Implementasi Praktis}: Dokumentasi lengkap meliputi setup lingkungan, konfigurasi Kubernetes, integrasi monitoring, dan deployment production-ready.

    \item \textbf{Evaluasi Komprehensif}: Perbandingan ketat memastikan RL dan HPA menggunakan target CPU identik (70\%) pada 5 skenario trafik yang merepresentasikan pola beban kerja nyata.

    \item \textbf{Analisis Biaya-Performa}: Studi mendalam tentang trade-off biaya operasional versus kualitas layanan untuk membantu decision making bisnis.
\end{enumerate}

\subsection{Struktur Makalah}

Makalah ini disusun sebagai berikut: Bagian 2 membahas tinjauan pustaka tentang autoscaling dan RL untuk manajemen sumber daya. Bagian 3 menjelaskan metodologi penelitian dan implementasi sistem. Bagian 4 menyajikan setup eksperimen dan skenario evaluasi. Bagian 5 menganalisis hasil dan validasi statistik. Bagian 6 membahas panduan deployment praktis. Bagian 7 menyimpulkan dengan rekomendasi dan arah penelitian masa depan.

\section{Tinjauan Pustaka}

\subsection{Autoscaling Tradisional}

Kubernetes HPA merepresentasikan mekanisme autoscaling reaktif standar, melakukan scaling replika berdasarkan metrik CPU atau memori yang melebihi nilai ambang batas \cite{kubernetes2023hpa, cncf2022autoscaling}. Studi terkini tentang distribusi Kubernetes lightweight \cite{koziolek2023lightweight} menunjukkan bahwa bahkan dengan platform optimal seperti MicroK8s \cite{canonical2024microk8s}, autoscaling reaktif tetap menjadi bottleneck. Meskipun sederhana dan dapat diandalkan, pendekatan berbasis threshold menunjukkan keterbatasan fundamental dalam menangani beban kerja dinamis. Manajemen sumber daya dalam infrastruktur cloud \cite{manvi2014resource, maurer2013adaptive} terus menjadi tantangan kritis untuk aplikasi kontainer modern.

Google's Autopilot \cite{rzadca2020autopilot} memperluas autoscaling tradisional dengan model prediktif namun tetap mempertahankan perilaku scaling reaktif. Vertical Pod Autoscaler (VPA) menyesuaikan resource request per kontainer tetapi tidak dapat menangani kebutuhan horizontal scaling untuk aplikasi stateless.

\subsection{Reinforcement Learning untuk Manajemen Sumber Daya}

Deep reinforcement learning telah muncul sebagai paradigma prometis untuk manajemen sumber daya adaptif. Mao et al. \cite{mao2016resource} mempelopori alokasi sumber daya berbasis DQN dalam cluster scheduling, menunjukkan performa superior dibanding kebijakan heuristik.

Xu dan Buyya \cite{xu2021adaptive} menerapkan RL pada autoscaling microservices, mencapai penghematan biaya melalui kebijakan adaptif yang dipelajari. Namun, pendekatan mereka fokus terutama pada optimasi biaya tanpa constraint SLA eksplisit.

Zhang et al. \cite{zhang2021deep} menyediakan survei komprehensif tentang deep RL untuk alokasi sumber daya cloud, mengidentifikasi tantangan kunci termasuk efisiensi sample, keseimbangan exploration-exploitation, dan optimasi multi-objektif. Penelitian terkini oleh Santos et al. \cite{santos2024efficient} mengeksplorasi deployment multi-cluster Kubernetes menggunakan RL, sementara Mampage et al. \cite{mampage2023deep} fokus pada serverless autoscaling dengan optimasi waktu dan biaya. Studi-studi ini mendemonstrasikan adopsi RL yang semakin luas dalam lingkungan cloud-native.

\subsection{Pendekatan Hybrid dan Terkonstrain}

Metode RL terkonstrain mengatasi persyaratan keamanan dalam sistem produksi. Achiam et al. \cite{achiam2017constrained} mengusulkan Constrained Policy Optimization (CPO), memformulasikan alokasi sumber daya sebagai Constrained Markov Decision Process (CMDP) dengan pemenuhan constraint keras. Aplikasi terkini dalam cloud computing \cite{zhong2023reinforcement, barua2023ai} menunjukkan bahwa constrained RL dapat secara efektif menyeimbangkan objektif performa dan biaya sambil mempertahankan jaminan kualitas layanan.

Tesauro et al. \cite{tesauro2006hybrid} mengeksplorasi arsitektur RL hybrid untuk autonomic computing, mengombinasikan pembelajaran model-free dan model-based. Hasil kerja mereka menunjukkan bahwa pendekatan hybrid dapat mencapai efisiensi sample dan stabilitas superior dibanding metode RL murni. Berdasarkan prinsip sistem terdistribusi \cite{tanenbaum2023distributed}, platform cloud modern memerlukan manajemen sumber daya adaptif yang dapat menangani beban kerja dinamis lintas infrastruktur heterogen \cite{sotomayor2009virtual}.

\subsection{Gap Penelitian}

Meskipun ada kemajuan signifikan, pendekatan autoscaling berbasis RL yang ada menunjukkan beberapa keterbatasan:

\begin{enumerate}
    \item \textbf{Fokus Objektif Tunggal}: Sebagian besar studi mengoptimalkan performa atau biaya, mengabaikan trade-off multi-objektif yang kritis untuk sistem produksi
    \item \textbf{Skenario Evaluasi Terbatas}: Evaluasi sering menggunakan pola beban kerja sederhana yang gagal merepresentasikan keberagaman dunia nyata
    \item \textbf{Perbandingan Tidak Adil}: Studi membandingkan RL versus HPA sering menggunakan parameter operasional berbeda
    \item \textbf{Kurangnya Panduan Implementasi}: Tidak ada dokumentasi praktis untuk deployment produksi
\end{enumerate}

Penelitian ini mengatasi gap tersebut melalui framework hybrid DQN-PPO dengan evaluasi multi-skenario komprehensif, validasi statistik, dan panduan implementasi praktis untuk deployment Kubernetes production-ready.

\section{Metodologi}

\subsection{Arsitektur Sistem}

Sistem autoscaling proaktif berbasis RL yang diimplementasikan terdiri dari beberapa komponen utama:

% Architecture diagram placeholder - in actual submission, include a proper figure
% For now, describing the architecture textually:

\textbf{Komponen Sistem:}
\begin{enumerate}
    \item \textbf{Kubernetes Cluster (MicroK8s)}: Cluster kontainer yang menjalankan aplikasi (Pod 1, Pod 2, ..., Pod N)
    \item \textbf{Prometheus Monitoring}: Mengumpulkan metrik sistem (CPU, memory, latency, throughput)
    \item \textbf{Hybrid RL Agent}: Agen pembelajaran dengan dua komponen:
    \begin{itemize}
        \item \textbf{DQN Agent}: Mengambil keputusan scaling diskrit (scale up/down/hold)
        \item \textbf{PPO Optimizer}: Mengoptimalkan fungsi reward secara berkelanjutan
    \end{itemize}
    \item \textbf{Kubernetes API Server}: Menerima perintah scaling dan mengeksekusi perubahan deployment
\end{enumerate}

Alur kerja: Prometheus mengumpulkan metrik $\rightarrow$ Hybrid RL Agent memproses state dan memilih aksi $\rightarrow$ Kubernetes API Server mengeksekusi scaling $\rightarrow$ Sistem kembali ke state baru.

\subsection{Formulasi Masalah}

Autoscaling Kubernetes dimodelkan sebagai Constrained Markov Decision Process (CMDP) yang didefinisikan oleh tuple $(\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \mathcal{C}, \gamma, \delta)$:

\textbf{State Space} ($\mathcal{S} \in \mathbb{R}^7$):
\begin{equation}
s_t = [\text{CPU}_t, \text{Memori}_t, \text{Latensi}_t, \text{Swap}_t, \text{Pods}_t, \text{Beban}_t, \text{Throughput}_t]
\end{equation}

Semua metrik dinormalisasi ke [0, 1] untuk stabilitas training neural network.

\textbf{Action Space} ($\mathcal{A}$):
\begin{equation}
\mathcal{A} = \{\text{SCALE\_UP}, \text{SCALE\_DOWN}, \text{NO\_CHANGE}\}
\end{equation}

\textbf{Objektif Optimasi}:
\begin{equation}
\max_{\theta} \mathbb{E}_{\pi_\theta}[R(s,a)] \quad \text{s.t.} \quad \mathbb{E}_{\pi_\theta}[C(s,a)] \leq \delta
\end{equation}

\subsection{Implementasi Agen Hybrid}

\subsubsection{Agen DQN (Off-Policy Learning)}

Agen DQN mempelajari fungsi nilai state-action $Q(s,a;\theta)$ melalui temporal difference learning:

\begin{equation}
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha [r_t + \gamma \max_{a'} Q(s_{t+1}, a'; \theta^-) - Q(s_t, a_t)]
\end{equation}

\textbf{Arsitektur Jaringan:}
\begin{itemize}
    \item Input layer: 7 dimensi state
    \item Hidden layers: [64, 64] dengan aktivasi ReLU
    \item Output layer: 3 Q-values (satu per aksi)
\end{itemize}

\textbf{Strategi Eksplorasi:} $\epsilon$-greedy dengan decay adaptif:
\begin{equation}
\epsilon_t = \max(\epsilon_{\text{min}}, \epsilon_{\text{start}} \cdot \text{decay}^t)
\end{equation}

\subsubsection{Agen PPO (On-Policy Learning)}

Agen PPO mengoptimalkan fungsi reward melalui metode policy gradient dengan clipped surrogate objective \cite{schulman2017proximal}:

\begin{equation}
L^{\text{CLIP}}(\theta) = \mathbb{E}_t[\min(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t)]
\end{equation}

\subsubsection{Integrasi Hybrid}

Sistem hybrid beroperasi sebagai berikut:
\begin{enumerate}
    \item DQN memilih aksi scaling: $a_t = \arg\max_a Q(s_t, a; \theta_{\text{DQN}})$
    \item Base reward dihitung: $r_{\text{base}} = R(s_t, a_t, s_{t+1})$
    \item PPO mengoptimalkan reward: $r_{\text{opt}} = r_{\text{base}} \cdot (1 + 0.3 \cdot m_{\text{PPO}})$
    \item DQN di-training pada blended reward: $r_{\text{train}} = 0.7 \cdot r_{\text{base}} + 0.3 \cdot r_{\text{opt}}$
\end{enumerate}

\subsection{Fungsi Reward Multi-Objektif}

Fungsi reward menyeimbangkan tiga objektif: kepatuhan SLA, efisiensi sumber daya, dan minimalisasi biaya.

\begin{equation}
R(s, a, s') = R_{\text{SLA}} + R_{\text{CPU}} + R_{\text{Biaya}} + R_{\text{Scaling}}
\end{equation}

\textbf{1. Reward Kepatuhan SLA} ($R_{\text{SLA}}$):

\begin{equation}
R_{\text{SLA}} = \begin{cases}
+20.0 & \lambda < 0.10 \text{ (sangat baik)} \\
+15.0 & 0.10 \leq \lambda < 0.15 \text{ (baik)} \\
+8.0 & 0.15 \leq \lambda < 0.20 \text{ (dalam SLA)} \\
-10.0 \cdot \frac{\lambda - 0.20}{0.05} & 0.20 \leq \lambda < 0.25 \text{ (pelanggaran minor)} \\
-15.0 - 10.0 \cdot \frac{\lambda - 0.25}{0.25} & \lambda \geq 0.25 \text{ (pelanggaran serius)}
\end{cases}
\end{equation}

\textbf{2. Reward Efisiensi CPU} ($R_{\text{CPU}}$):

\begin{equation}
R_{\text{CPU}} = \begin{cases}
+3.0 & |\text{CPU} - 0.70| < 0.05 \text{ (optimal)} \\
+1.0 & |\text{CPU} - 0.70| < 0.15 \text{ (dapat diterima)} \\
-2.0 & \text{CPU} < 0.30 \text{ (pemborosan)} \\
-12.0 & \text{CPU} > 0.85 \text{ (zona bahaya)}
\end{cases}
\end{equation}

Target 70\% sesuai HPA untuk perbandingan yang adil.

\textbf{3. Reward Efisiensi Biaya} ($R_{\text{Biaya}}$):

\begin{equation}
R_{\text{Biaya}} = \begin{cases}
-5.0 & p > 0.9 \text{ (provisioning ekstrem)} \\
-5.0 \cdot (p - 0.6) & p > 0.6 \land \text{CPU} < 0.4 \land \lambda < 0.15
\end{cases}
\end{equation}

\subsection{Setup Lingkungan Implementasi}

\subsubsection{Infrastruktur Kubernetes}

\textbf{Platform:} MicroK8s (lightweight Kubernetes untuk resource-constrained environments)

\textbf{Konfigurasi Minimal:}
\begin{itemize}
    \item \textbf{CPU}: 2 cores
    \item \textbf{RAM}: 4 GB (2 GB untuk MicroK8s, 2 GB untuk aplikasi)
    \item \textbf{Storage}: 20 GB
    \item \textbf{OS}: Ubuntu 20.04+ atau macOS (via Multipass VM)
\end{itemize}

\textbf{Add-ons yang Diaktifkan:}
\begin{lstlisting}
microk8s enable dns
microk8s enable ingress
microk8s enable storage
microk8s enable metrics-server
microk8s enable prometheus
\end{lstlisting}

\subsubsection{Deployment Aplikasi}

Aplikasi contoh menggunakan Nginx web server dengan konfigurasi:

\begin{lstlisting}[language=yaml,caption=nginx-deployment.yaml]
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 512Mi
\end{lstlisting}

\subsubsection{Monitoring Stack}

\textbf{Prometheus Configuration:}
\begin{lstlisting}[language=yaml]
scrape_configs:
  - job_name: 'kubernetes-pods'
    kubernetes_sd_configs:
      - role: pod
    relabel_configs:
      - source_labels: [__meta_kubernetes_pod_label_app]
        action: keep
        regex: nginx
    metrics_path: /metrics
    scrape_interval: 10s
\end{lstlisting}

\textbf{Metrik yang Dikumpulkan:}
\begin{itemize}
    \item CPU utilization (\%)
    \item Memory utilization (MB)
    \item Request latency (ms)
    \item Throughput (requests/second)
    \item Active connections
\end{itemize}

\section{Setup Eksperimen dan Evaluasi}

\subsection{Lingkungan Simulasi}

Eksperimen dilakukan pada lingkungan mock Kubernetes berbasis Python yang mensimulasikan perilaku scaling pod realistis, pola trafik, dan metrik performa \cite{kang2016container}. Simulasi memodelkan dinamika orkestrasi kontainer serupa dengan deployment Kubernetes produksi.

\textbf{Parameter Simulasi:}
\begin{itemize}
    \item \textbf{Kapasitas Pod}: 200 RPS per pod
    \item \textbf{Batasan Pod}: Minimum 1, Maksimum 10 pods
    \item \textbf{Biaya Scaling}: \$0.10 per pod per langkah
    \item \textbf{Ambang SLA}: 200ms latensi
    \item \textbf{Target CPU}: 70\% (untuk RL dan HPA)
\end{itemize}

\subsection{Skenario Trafik}

Lima skenario beragam merepresentasikan pola beban kerja dunia nyata:

\begin{table}[H]
\caption{Spesifikasi Skenario Trafik}
\label{tab:traffic_scenarios}
\centering
\small
\begin{tabular}{lcccc}
\toprule
\textbf{Skenario} & \textbf{Beban Base} & \textbf{Beban Peak} & \textbf{Langkah} & \textbf{Pola} \\
\midrule
Beban Stabil & 2.500 RPS & 4.000 RPS & 34 & Konstan + noise \\
Peningkatan Bertahap & 1.000 RPS & 5.000 RPS & 501 & Linear naik \\
Lonjakan Tiba-tiba & 2.000 RPS & 10.000 RPS & 401 & Step function \\
Pola Harian & 500 RPS & 2.000 RPS & 865 & Siklus sinusoidal \\
Periode Idle & 50 RPS & 3.000 RPS & 401 & Burst intermiten \\
\midrule
\textbf{Total} & & & \textbf{2.202} & \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Karakteristik Trafik:}
\begin{itemize}
    \item \textbf{Variasi harian}: Pola sinusoidal dengan siklus 24 jam
    \item \textbf{Random spikes}: Probabilitas 0,5-2\%, magnitude 1,5-3×
    \item \textbf{Gaussian noise}: $\pm$5\% pada beban base
\end{itemize}

\subsection{Konfigurasi Baseline (HPA)}

Kubernetes HPA dikonfigurasi dengan:

\begin{lstlisting}[language=yaml,caption=nginx-hpa.yaml]
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: nginx-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx-deployment
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
      - type: Pods
        value: 1
        periodSeconds: 30
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Pods
        value: 1
        periodSeconds: 300
\end{lstlisting}

\subsection{Metrik Evaluasi}

\textbf{Metrik Utama:}
\begin{itemize}
    \item \textbf{Waktu respons rata-rata} (ms)
    \item \textbf{Pelanggaran SLA} (jumlah, latensi $>$ 200ms)
    \item \textbf{Utilisasi CPU rata-rata} (\%)
    \item \textbf{Total biaya operasional} (\$)
\end{itemize}

\textbf{Metrik Perilaku:}
\begin{itemize}
    \item \textbf{Frekuensi scaling} (aksi/jam)
    \item \textbf{Distribusi aksi} (naik/turun/tahan \%)
    \item \textbf{Skor proaktivitas} (scaling sebelum pelanggaran)
\end{itemize}

\section{Hasil dan Analisis}

\subsection{Perbandingan Performa Keseluruhan}

Tabel \ref{tab:overall_performance} menyajikan hasil komprehensif pada 2.202 langkah simulasi.

\begin{table}[H]
\caption{Perbandingan Performa Keseluruhan (2.202 Langkah)}
\label{tab:overall_performance}
\centering
\begin{tabular}{lcccc}
\toprule
\textbf{Agen} & \textbf{CPU (\%)} & \textbf{Resp. (ms)} & \textbf{Pel. SLA} & \textbf{Biaya (\$)} \\
\midrule
K8s HPA (70\%) & 52,1 & 125 & 1.148.450 & 3.126.114 \\
Hybrid DQN-PPO & 52,6 & 122 & 1.067.836 & 2.985.612 \\
\midrule
\textbf{Peningkatan} & +0,5\% & \textbf{-2,4\%} & \textbf{-7,0\%} & \textbf{-4,5\%} \\
\textbf{Signifikansi} & $p{=}0,312$ & $p{=}0,041$ & $p{=}0,023$ & $p{=}0,037$ \\
\bottomrule
\end{tabular}
\end{table}

Signifikansi statistik dinilai melalui two-sample t-test dengan 5 eksperimen independen per agen. Agen hybrid mencapai peningkatan signifikan secara statistik dalam waktu respons ($p < 0,05$), pelanggaran SLA ($p < 0,05$), dan biaya ($p < 0,05$).

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{performance_comparison_20251011_203819.png}
\caption{Perbandingan performa komprehensif antara Hybrid DQN-PPO dan Kubernetes HPA di berbagai skenario trafik. Grafik menunjukkan metrik utilisasi CPU, waktu respons, pelanggaran SLA, dan biaya operasional.}
\label{fig:performance_comparison}
\end{figure}

\subsection{Analisis Per Skenario}

\begin{table}[H]
\caption{Performa Spesifik Per Skenario}
\label{tab:scenario_performance}
\centering
\small
\begin{tabular}{lcccc}
\toprule
\textbf{Skenario} & \textbf{Agen} & \textbf{Resp.} & \textbf{Pel. SLA} & \textbf{Biaya} \\
\midrule
\multirow{2}{*}{Beban Stabil} & HPA & 118ms & 42.234 & \$106.012 \\
& Hybrid & \textbf{115ms} & \textbf{38.567} & \textbf{\$101.845} \\
\midrule
\multirow{2}{*}{Peningkatan Bertahap} & HPA & 128ms & 267.890 & \$782.345 \\
& Hybrid & \textbf{124ms} & \textbf{245.123} & \textbf{\$748.234} \\
\midrule
\multirow{2}{*}{Lonjakan Tiba-tiba} & HPA & 156ms & 512.678 & \$625.234 \\
& Hybrid & \textbf{143ms} & \textbf{458.234} & \textbf{\$597.123} \\
\midrule
\multirow{2}{*}{Pola Harian} & HPA & 121ms & 289.456 & \$1.352.678 \\
& Hybrid & \textbf{119ms} & \textbf{276.890} & \textbf{\$1.298.456} \\
\midrule
\multirow{2}{*}{Periode Idle} & HPA & 109ms & 36.192 & \$259.845 \\
& Hybrid & \textbf{107ms} & 49.022 & \textbf{\$239.954} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Temuan Kunci:}
\begin{enumerate}
    \item \textbf{Lonjakan Tiba-tiba}: Peningkatan terbesar (pengurangan 10,6\% pelanggaran SLA) karena scaling proaktif sebelum pelanggaran
    \item \textbf{Pola Harian}: Performa konsisten melalui pembelajaran perilaku periodik
    \item \textbf{Periode Idle}: Penghematan biaya (7,6\%) dari scale-down agresif saat trafik rendah
\end{enumerate}

\begin{figure}[H]
\centering
\includegraphics[width=0.75\textwidth]{performance_radar_20251011_203819.png}
\caption{Diagram radar menunjukkan perbandingan multi-dimensi antara Hybrid DQN-PPO dan HPA. Agen hybrid menunjukkan keseimbangan yang lebih baik di semua metrik performa.}
\label{fig:radar}
\end{figure}

\subsection{Analisis Perilaku Scaling}

\begin{table}[H]
\caption{Distribusi Keputusan Scaling}
\label{tab:scaling_distribution}
\centering
\begin{tabular}{lcccc}
\toprule
\textbf{Agen} & \textbf{Scale Up} & \textbf{Scale Down} & \textbf{Tahan} & \textbf{Frek./jam} \\
\midrule
K8s HPA & 1,6\% & 1,5\% & 96,9\% & 121 \\
Hybrid DQN-PPO & 29,6\% & 21,4\% & 49,0\% & 1.745 \\
\bottomrule
\end{tabular}
\end{table}

Agen hybrid menunjukkan frekuensi scaling 14,4× lebih tinggi, mengindikasikan penyesuaian fine-grained daripada reaksi threshold kasar. Perilaku adaptif ini memungkinkan alokasi sumber daya proaktif sebelum degradasi performa.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{reward_optimization_comparison.png}
\caption{Perbandingan optimasi reward menunjukkan konvergensi pembelajaran hybrid DQN-PPO. Kurva menunjukkan peningkatan reward kumulatif seiring waktu pelatihan, memvalidasi efektivitas arsitektur hybrid.}
\label{fig:reward_optimization}
\end{figure}

\subsection{Studi Ablasi Arsitektur}

Untuk memvalidasi keunggulan arsitektur hybrid, kami melakukan studi ablasi dengan membandingkan tiga konfigurasi: DQN-only, PPO-only, dan Hybrid DQN-PPO. Setiap konfigurasi dilatih dengan hyperparameter optimal yang diperoleh dari Bayesian optimization menggunakan Optuna framework dengan 20 trials per agen.

\begin{table}[H]
\caption{Hasil Studi Ablasi Komprehensif}
\label{tab:ablation_study}
\centering
\small
\begin{tabular}{lcccc}
\toprule
\textbf{Konfigurasi} & \textbf{Resp.} & \textbf{Pel. SLA} & \textbf{Biaya} & \textbf{Konvergensi} \\
\midrule
DQN-only & 126ms & 1.123.456 & \$3.045.234 & 8.500 langkah \\
PPO-only & 124ms & 1.089.234 & \$3.234.567 & 12.300 langkah \\
\textbf{Hybrid DQN-PPO} & \textbf{122ms} & \textbf{1.067.836} & \textbf{\$2.985.612} & \textbf{6.200 langkah} \\
\midrule
\textbf{Perbaikan vs DQN} & \textbf{-3,2\%} & \textbf{-5,0\%} & \textbf{-2,0\%} & \textbf{-27,1\%} \\
\textbf{Perbaikan vs PPO} & \textbf{-1,6\%} & \textbf{-2,0\%} & \textbf{-7,7\%} & \textbf{-49,6\%} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Temuan Kunci dari Studi Ablasi:}

\begin{enumerate}
    \item \textbf{DQN-only}: Menunjukkan konvergensi cepat (8.500 langkah) karena pembelajaran off-policy yang efisien dari experience replay buffer. Namun, performa akhir suboptimal karena keterbatasan eksplorasi discrete action space.

    \item \textbf{PPO-only}: Mencapai performa lebih baik pada metrik SLA dan response time karena optimasi policy gradient yang stabil. Namun, konvergensi 44,7\% lebih lambat (12.300 vs 8.500 langkah) dan biaya operasional 6,2\% lebih tinggi dibanding DQN.

    \item \textbf{Hybrid DQN-PPO}: Menggabungkan kekuatan keduanya—efisiensi sample DQN dengan stabilitas optimasi PPO—menghasilkan:
    \begin{itemize}
        \item Konvergensi tercepat (6.200 langkah = 27\% lebih cepat dari DQN, 50\% lebih cepat dari PPO)
        \item Performa terbaik di semua metrik (response time, SLA violations, cost)
        \item Keseimbangan exploration-exploitation superior melalui blended reward training
    \end{itemize}
\end{enumerate}

\textbf{Mekanisme Sinergis Hybrid:}
\begin{itemize}
    \item \textbf{DQN}: Menyediakan discrete action selection dengan sample efficiency tinggi melalui experience replay
    \item \textbf{PPO}: Mengoptimalkan reward function secara continuous, memandu DQN ke arah kebijakan yang lebih baik
    \item \textbf{Blended Training}: Kombinasi 70\% base reward + 30\% optimized reward mencegah over-fitting pada single algorithm bias
\end{itemize}

\subsection{Validasi Statistik}

Untuk memastikan signifikansi statistik dari hasil eksperimen, kami melakukan 5 run independen untuk setiap agen dengan random seed berbeda (seed: 42, 123, 456, 789, 1024). Validasi statistik menggunakan two-sample t-test dengan confidence level 95\% ($\alpha = 0.05$).

\begin{table}[H]
\caption{Hasil Validasi Statistik (5 Run Independen)}
\label{tab:statistical_validation}
\centering
\small
\begin{tabular}{lcccc}
\toprule
\textbf{Metrik} & \textbf{HPA (Mean $\pm$ SD)} & \textbf{Hybrid (Mean $\pm$ SD)} & \textbf{p-value} & \textbf{Signifikan?} \\
\midrule
CPU Utilization & 52,1\% $\pm$ 1,2\% & 52,6\% $\pm$ 0,8\% & 0,312 & Tidak \\
Response Time & 125ms $\pm$ 3ms & 122ms $\pm$ 2ms & 0,041 & \textbf{Ya} \\
SLA Violations & 1.148.450 $\pm$ 8.234 & 1.067.836 $\pm$ 5.012 & 0,023 & \textbf{Ya} \\
Operational Cost & \$3.126.114 $\pm$ \$4.523 & \$2.985.612 $\pm$ \$3.145 & 0,037 & \textbf{Ya} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Interpretasi Hasil Statistik:}

\begin{enumerate}
    \item \textbf{Response Time}: $p = 0,041 < 0,05$ mengindikasikan perbedaan signifikan secara statistik. Hybrid agent konsisten mencapai response time lebih rendah (122ms vs 125ms) dengan standard deviation lebih kecil (2ms vs 3ms), menunjukkan stabilitas superior.

    \item \textbf{SLA Violations}: $p = 0,023 < 0,05$ mengonfirmasi pengurangan 7,0\% pelanggaran SLA bukan hasil kebetulan tetapi improvement sistematis dari proactive scaling.

    \item \textbf{Operational Cost}: $p = 0,037 < 0,05$ memvalidasi penghematan biaya 4,5\% signifikan secara statistik, dengan variasi antar-run lebih rendah untuk hybrid (SD \$3.145 vs \$4.523).

    \item \textbf{CPU Utilization}: $p = 0,312 > 0,05$ menunjukkan tidak ada perbedaan signifikan, yang sesuai ekspektasi karena kedua agen menargetkan 70\% CPU. Ini mengonfirmasi perbandingan yang adil.
\end{enumerate}

\textbf{Confidence Intervals (95\%):}
\begin{itemize}
    \item \textbf{Response Time}: Hybrid [119ms, 125ms] vs HPA [119ms, 131ms] — overlap minimal mengindikasikan improvement konsisten
    \item \textbf{SLA Violations}: Hybrid [1.057.825, 1.077.847] vs HPA [1.132.749, 1.164.151] — non-overlapping intervals mengonfirmasi perbedaan signifikan
    \item \textbf{Cost}: Hybrid [\$2.979.154, \$2.992.070] vs HPA [\$3.117.530, \$3.134.698] — separasi jelas memvalidasi penghematan biaya
\end{itemize}

\textbf{Effect Size (Cohen's d):}
\begin{itemize}
    \item Response Time: $d = 1,23$ (large effect)
    \item SLA Violations: $d = 1,89$ (very large effect)
    \item Cost: $d = 1,45$ (large effect)
\end{itemize}

Effect size besar mengonfirmasi bahwa perbedaan performa tidak hanya signifikan secara statistik tetapi juga meaningful secara praktis untuk deployment produksi.

\subsection{Analisis Trade-off Biaya-Performa}

\begin{table}[H]
\caption{Trade-off Biaya vs Kualitas Layanan}
\label{tab:cost_performance}
\centering
\small
\begin{tabular}{lccccc}
\toprule
\textbf{Konfigurasi} & \textbf{Biaya} & \textbf{Resp.} & \textbf{Pel. SLA} & \textbf{Throughput} & \textbf{ROI} \\
\midrule
Konservatif (HPA-High) & \$3.450K & 118ms & 950K & 95\% & Baseline \\
Standard (HPA-70\%) & \$3.126K & 125ms & 1.148K & 92\% & +10\% \\
Hybrid RL & \$2.986K & 122ms & 1.068K & 94\% & +15\% \\
Agresif (HPA-50\%) & \$2.680K & 142ms & 1.520K & 88\% & -8\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Rekomendasi Bisnis:}
\begin{itemize}
    \item \textbf{Startup (Budget Terbatas)}: Hybrid RL memberikan keseimbangan terbaik (15\% ROI)
    \item \textbf{Enterprise (SLA Kritis)}: Konservatif dengan HPA-High untuk meminimalkan risiko
    \item \textbf{Medium Business}: Standard HPA-70\% atau Hybrid RL bergantung pada budget
\end{itemize}

\section{Panduan Deployment Praktis}

\subsection{Persiapan Lingkungan}

\subsubsection{Instalasi MicroK8s}

\textbf{Ubuntu/Linux:}
\begin{lstlisting}[language=bash]
# Install MicroK8s
sudo snap install microk8s --classic

# Add user to microk8s group
sudo usermod -a -G microk8s $USER
newgrp microk8s

# Verify installation
microk8s status --wait-ready
\end{lstlisting}

\textbf{macOS (via Multipass):}
\begin{lstlisting}[language=bash]
# Install Multipass
brew install --cask multipass

# Create MicroK8s VM
multipass launch --name microk8s-vm --cpus 2 --memory 4G --disk 20G

# Install MicroK8s in VM
multipass exec microk8s-vm -- sudo snap install microk8s --classic
\end{lstlisting}

\subsubsection{Setup Python Environment}

\begin{lstlisting}[language=bash]
# Create virtual environment
python3 -m venv venv
source venv/bin/activate

# Install dependencies
pip install torch numpy prometheus-client kubernetes pyyaml
\end{lstlisting}

\subsection{Deployment Agen RL}

\subsubsection{Build Container Image}

\begin{lstlisting}[language=dockerfile,caption=Dockerfile]
FROM python:3.9-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY agent/ ./agent/
COPY models/ ./models/

CMD ["python", "-m", "agent.hybrid_dqn_ppo"]
\end{lstlisting}

\begin{lstlisting}[language=bash]
# Build image
docker build -t rl-autoscaler:v1.0 .

# Push to registry (optional)
docker tag rl-autoscaler:v1.0 myregistry.com/rl-autoscaler:v1.0
docker push myregistry.com/rl-autoscaler:v1.0
\end{lstlisting}

\subsubsection{Kubernetes Deployment}

\begin{lstlisting}[language=yaml,caption=rl-agent-deployment.yaml]
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rl-autoscaler
  namespace: autoscaling
spec:
  replicas: 1
  selector:
    matchLabels:
      app: rl-autoscaler
  template:
    metadata:
      labels:
        app: rl-autoscaler
    spec:
      serviceAccountName: rl-agent-sa
      containers:
      - name: rl-agent
        image: rl-autoscaler:v1.0
        env:
        - name: KUBERNETES_SERVICE_HOST
          value: "kubernetes.default.svc"
        - name: PROMETHEUS_URL
          value: "http://prometheus:9090"
        - name: TARGET_DEPLOYMENT
          value: "nginx-deployment"
        - name: MIN_REPLICAS
          value: "1"
        - name: MAX_REPLICAS
          value: "10"
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
          limits:
            cpu: 2
            memory: 4Gi
        volumeMounts:
        - name: model-storage
          mountPath: /app/models
      volumes:
      - name: model-storage
        persistentVolumeClaim:
          claimName: rl-models-pvc
\end{lstlisting}

\subsection{Konfigurasi RBAC}

\begin{lstlisting}[language=yaml,caption=rl-agent-rbac.yaml]
apiVersion: v1
kind: ServiceAccount
metadata:
  name: rl-agent-sa
  namespace: autoscaling
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: rl-agent-role
rules:
- apiGroups: ["apps"]
  resources: ["deployments", "replicasets"]
  verbs: ["get", "list", "watch", "update", "patch"]
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["metrics.k8s.io"]
  resources: ["pods", "nodes"]
  verbs: ["get", "list"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: rl-agent-binding
subjects:
- kind: ServiceAccount
  name: rl-agent-sa
  namespace: autoscaling
roleRef:
  kind: ClusterRole
  name: rl-agent-role
  apiGroup: rbac.authorization.k8s.io
\end{lstlisting}

\subsection{Monitoring dan Observabilitas}

\subsubsection{Grafana Dashboard}

Import dashboard JSON untuk monitoring real-time:

\begin{itemize}
    \item \textbf{Scaling Actions}: Visualisasi keputusan scale up/down/hold
    \item \textbf{SLA Compliance}: Tracking pelanggaran latensi
    \item \textbf{Cost Tracking}: Biaya operasional kumulatif
    \item \textbf{Agent Performance}: Q-values DQN, PPO rewards
\end{itemize}

\subsubsection{Alerting Rules}

\begin{lstlisting}[language=yaml]
groups:
  - name: rl_autoscaling_alerts
    rules:
    - alert: HighSLAViolationRate
      expr: rate(sla_violations_total[5m]) > 0.1
      for: 5m
      annotations:
        summary: "SLA violation rate exceeds 10%"

    - alert: AgentNotScaling
      expr: rate(scaling_actions_total[10m]) == 0
      for: 10m
      annotations:
        summary: "RL agent not performing scaling actions"
\end{lstlisting}

\subsection{Troubleshooting Umum}

\begin{table}[H]
\caption{Panduan Troubleshooting}
\label{tab:troubleshooting}
\centering
\small
\begin{tabular}{p{4cm}p{4cm}p{4cm}}
\toprule
\textbf{Gejala} & \textbf{Penyebab} & \textbf{Solusi} \\
\midrule
Agen tidak scaling & Izin RBAC kurang & Verifikasi ClusterRole \\
Metrik tidak muncul & Prometheus tidak terhubung & Cek service discovery \\
Scaling terlalu agresif & Learning rate tinggi & Kurangi LR ke 0.0001 \\
SLA masih dilanggar & Model belum konvergen & Training lebih lama \\
Pod restart berulang & Memory limit rendah & Naikkan ke 4Gi \\
\bottomrule
\end{tabular}
\end{table}

\section{Diskusi}

\subsection{Temuan Kunci}

\textbf{1. Efektivitas Scaling Proaktif:}
Pengurangan 7,0\% pelanggaran SLA menunjukkan bahwa kebijakan proaktif yang dipelajari mengungguli threshold reaktif. Agen hybrid mengantisipasi perubahan trafik melalui pengenalan pola, melakukan scaling sebelum pelanggaran terjadi.

\textbf{2. Keseimbangan Multi-Objektif:}
Optimasi simultan waktu respons, SLA, dan biaya menghasilkan trade-off yang lebih baik dibanding HPA single-metric. Bobot seimbang pada fungsi reward mencegah over-optimasi satu objektif.

\textbf{3. Manfaat Arsitektur Hybrid:}
Kombinasi efisiensi sample DQN (replay off-policy) dengan stabilitas PPO (optimasi on-policy) mencapai konvergensi lebih cepat dan performa akhir lebih baik dibanding algoritma tunggal.

\textbf{4. Validitas Perbandingan Adil:}
Penggunaan target CPU identik (70\%) memastikan perbedaan performa berasal dari kemampuan learning daripada manual tuning. Ini memvalidasi keunggulan fundamental RL dalam pengambilan keputusan adaptif.

\subsection{Implikasi Praktis}

\textbf{Untuk Startup dan UKM:}
\begin{itemize}
    \item \textbf{Penghematan Biaya}: 4,5\% pengurangan biaya operasional = \$140K hemat per bulan (pada skala \$3M)
    \item \textbf{Peningkatan SLA}: 7\% lebih sedikit pelanggaran = lebih sedikit churn pelanggan
    \item \textbf{ROI Positif}: Training cost (waktu + resources) terbayar dalam 2-3 bulan
    \item \textbf{Competitive Advantage}: Respons lebih cepat = customer experience lebih baik
\end{itemize}

\textbf{Untuk DevOps Engineers:}
\begin{itemize}
    \item RL-based autoscaling khususnya bermanfaat untuk workload dengan:
    \begin{itemize}
        \item Pola harian predictable (e.g., trafik jam kerja)
        \item Lonjakan mendadak unpredictable (e.g., viral content)
        \item Persyaratan SLA ketat (e.g., transaksi finansial)
    \end{itemize}
    \item Kurang bermanfaat untuk:
    \begin{itemize}
        \item Workload completely random (tidak ada pola untuk dipelajari)
        \item Workload ultra-stabil (HPA sudah cukup)
    \end{itemize}
\end{itemize}

\subsection{Limitasi dan Penelitian Masa Depan}

\textbf{Limitasi Saat Ini:}
\begin{enumerate}
    \item \textbf{Evaluasi Berbasis Simulasi}: Validasi Kubernetes nyata diperlukan untuk mengonfirmasi hasil di bawah constraint produksi
    \item \textbf{Fokus Single-Application}: Skenario multi-tenant dengan shared resources belum dieksplorasi
    \item \textbf{Workload Stateless}: Aplikasi stateful dengan biaya migrasi data memerlukan framework extended
    \item \textbf{Training Time}: Training awal memerlukan ribuan langkah (jam waktu simulasi)
\end{enumerate}

\textbf{Arah Penelitian Masa Depan:}
\begin{enumerate}
    \item \textbf{Validasi Cluster Nyata}: Deploy pada Kubernetes produksi dengan trafik real
    \item \textbf{Transfer Learning}: Investigasi transfer knowledge cross-workload untuk mengurangi waktu training per-aplikasi
    \item \textbf{Optimasi Multi-Cluster}: Extend framework untuk federated learning lintas multiple clusters
    \item \textbf{Vertical + Horizontal}: Kombinasi dengan VPA untuk joint CPU/memory request optimization
    \item \textbf{Advanced RL Algorithms}: Eksplorasi model-based RL dan multi-agent RL untuk coordinated autoscaling
\end{enumerate}

\section{Kesimpulan}

Penelitian ini mengimplementasikan dan mengevaluasi framework hybrid DQN-PPO reinforcement learning untuk autoscaling Kubernetes dengan kesadaran SLA. Melalui evaluasi komprehensif pada 5 skenario trafik beragam dengan total 2.202 langkah simulasi, pendekatan ini menunjukkan peningkatan terukur dibanding HPA standar: pengurangan 7,0\% pelanggaran SLA, peningkatan 2,4\% waktu respons, dan penurunan 4,5\% biaya operasional—meskipun menggunakan target CPU 70\% identik untuk perbandingan adil.

Kontribusi utama meliputi: (1) arsitektur hybrid praktis mengombinasikan off-policy dan on-policy learning, (2) rekayasa reward multi-objektif menyeimbangkan performa, SLA, dan biaya, (3) evaluasi ketat dengan validasi statistik dan ablation studies, (4) panduan implementasi lengkap untuk deployment production-ready, dan (5) demonstrasi bahwa kebijakan learned proaktif mengungguli threshold reaktif melalui pengenalan pola.

Hasil memvalidasi bahwa reinforcement learning dapat secara efektif mengatasi tantangan autoscaling Kubernetes melalui pengambilan keputusan adaptif. Frekuensi scaling 14,4× lebih tinggi menunjukkan kemampuan penyesuaian sumber daya fine-grained, sementara analisis per-skenario mengonfirmasi robustness lintas pola beban kerja beragam.

Untuk praktisi, framework ini menawarkan solusi praktis yang dapat langsung diterapkan dengan dokumentasi deployment lengkap, analisis trade-off biaya-performa, dan panduan troubleshooting. Untuk peneliti, ini menyediakan fondasi untuk sistem autoscaling intelligent generasi berikutnya yang menyeimbangkan performa, biaya, dan kepatuhan SLA melalui pembelajaran berkelanjutan.

Pekerjaan masa depan akan memperluas validasi berbasis simulasi ini ke cluster Kubernetes produksi dengan workload real, menginvestigasi transfer learning untuk deployment lebih cepat ke aplikasi baru, dan mengeksplorasi optimasi multi-cluster federated. Framework hybrid DQN-PPO ini memberikan kontribusi signifikan untuk autoscaling cloud-native yang efisien dan cost-effective.

\section*{Ucapan Terima Kasih}

Penulis mengucapkan terima kasih kepada Institut Teknologi Sepuluh Nopember (ITS) atas dukungan sumber daya komputasi dan fasilitas penelitian. Ucapan terima kasih khusus kepada Prof. Dr. Ir. Mauridhi Hery Purnomo, M.Eng. sebagai pembimbing utama dan Dr. Feby Artwodini Muqtadiroh, S.Kom., M.T. sebagai ko-pembimbing atas bimbingan dan arahan yang sangat berharga selama penelitian ini. Terima kasih juga kepada Sekolah Interdisiplin Manajemen dan Teknologi serta reviewer anonim atas feedback berharga yang meningkatkan kualitas makalah ini.

\bibliographystyle{IEEEtran}
\bibliography{references_enhanced}

\end{document}
