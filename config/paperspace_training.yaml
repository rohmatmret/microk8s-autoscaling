# Paperspace Training Configuration
# Optimized for GPU-accelerated training on Paperspace Gradient

training:
  mode: "simulate"  # Always use simulation for safe cloud training
  total_steps: 100000
  checkpoint_freq: 5000
  eval_freq: 1000
  log_freq: 100

environment:
  deployment_name: "nginx-deployment"
  namespace: "default"
  max_pods: 10
  scaling_delay: 10
  metrics_sync_interval: 10
  prometheus_url: "http://localhost:9090"

dqn:
  learning_rate: 0.0005
  buffer_size: 100000
  batch_size: 64
  gamma: 0.99
  tau: 0.1
  epsilon_start: 1.0
  epsilon_end: 0.1
  epsilon_decay: 0.9995
  target_update_freq: 2000

ppo:
  learning_rate: 0.0003
  n_steps: 2048
  batch_size: 64
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  ent_coef: 0.01

hybrid:
  reward_optimization_freq: 100
  batch_evaluation_steps: 50
  state_dim: 7
  action_dim: 3
  hidden_dim: 64

wandb:
  project: "microk8s-autoscaling-paperspace"
  entity: "your-wandb-username"  # Change this to your WandB username
  tags: ["paperspace", "production", "gpu"]
  mode: "online"  # Enable real-time sync on Paperspace
